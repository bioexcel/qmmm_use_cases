CrayPat/X:  Version 6.4.6 Revision 7d0d87c  02/20/17 09:52:37

Number of PEs (MPI ranks):   32
                           
Numbers of PEs per Node:      4  PEs on each of  8  Nodes
                           
Numbers of Threads per PE:    6
                           
Number of Cores per Socket:  12

Execution start time:  Wed Mar  6 10:16:44 2019

System name and speed:  eslogin002  2701 MHz (approx)

Intel ivybridge CPU  Family:  6  Model: 62  Stepping:  4


Current path to data file:
  /work/d118/d118/fionabio/BioExcel2/cp2k_benchmarks/QMMM/QM/8_nodes/test-1/cp2k.psmp+samp+38276-4453s.ap2  (RTS)


Notes for table 1:

  Table option:
    -O samp_profile
  Options implied by table option:
    -d sa%@0.95,sa,imb_sa,imb_sa% -b gr,fu,pe=HIDE,th=HIDE

  Options for related tables:
    -O samp_profile+src        -O profile_max         

  The Total value for Samp is the sum of the Group values.
  The Group value for Samp is the sum of the Function values.
  The Function value for Samp is the avg of the PE values.
  The PE value for Samp is the value for the main thread only.
    (If the main thread is atypical, try the option -s aggr_th=sum.)
    (To specify different aggregations, see: pat_help report options s1)

  This table shows only lines with Samp% > 0.95.
    (To set thresholds to zero, specify:  -T)

  Imbalance percentages are relative to a set of threads or PEs.
  Other percentages at each level are of the Total for the program.
    (For percentages relative to next level up, specify:
      -s percent=r[elative])

  To make the profile easier to interpret, some samples are attributed
  to a caller that is either a user defined function, or a library
  function called directly by a user defined function.  To disable this
  adjustment, and show functions actually sampled, use the -P option.
  
  The following groups were pruned due to thresholding:
    IO, HEAP, PTHREAD, OMP, MATH, STDIO, BLAS, LAPACK, BLACS, SCALAPACK,
    FFTW, PBLAS, STRING, FS
  
  Trace option suggestions have been generated into a separate file
  from the data in the next table.  You can examine the file, edit it
  if desired, and use it to reinstrument the program like this:
  
            pat_build -O craypat_qm_8_nodes_32_mpiprocs_6_threads_5_steps.apa

Table 1:  Profile by Function

  Samp% |     Samp |  Imb. |  Imb. | Group
        |          |  Samp | Samp% |  Function
        |          |       |       |   PE=HIDE
        |          |       |       |    Thread=HIDE
       
 100.0% | 44,246.0 |    -- |    -- | Total
|-----------------------------------------------------------------------------
|  48.1% | 21,266.5 |    -- |    -- | USER
||----------------------------------------------------------------------------
||   8.5% |  3,773.8 | 193.2 |  5.0% | __pw_methods_MOD_pw_integral_ab
||   4.5% |  1,986.6 |  16.4 |  0.8% | __qmmm_gpw_energy_MOD_qmmm_elec_with_gaussian_lr
||   3.8% |  1,665.7 |  10.3 |  0.6% | __qmmm_gpw_forces_MOD_qmmm_force_with_gaussian_low
||   2.9% |  1,271.3 |  35.7 |  2.8% | __pw_spline_utils_MOD_pw_nn_compose_r_no_pbc._omp_fn.0
||   2.4% |  1,068.4 |  92.6 |  8.2% | __kahan_sum_MOD_kahan_sum_d3
||   2.3% |  1,031.0 |  50.0 |  4.8% | __fft_tools_MOD_x_to_yz._omp_fn.14
||   2.2% |    990.0 | 116.0 | 10.8% | __fast_MOD_zero_c2._omp_fn.1
||   1.9% |    835.1 |   7.9 |  1.0% | __pw_spline_utils_MOD_add_fine2coarse
||   1.8% |    803.3 |  43.7 |  5.3% | __pw_methods_MOD_pw_zero
||   1.8% |    782.3 |   7.7 |  1.0% | __pw_spline_utils_MOD_add_coarse2fine
||   1.5% |    682.3 |  88.7 | 11.9% | __pw_methods_MOD_pw_axpy
||   1.4% |    609.1 |  18.9 |  3.1% | __pw_methods_MOD_pw_gather_p._omp_fn.22
||   1.2% |    550.1 |  46.9 |  8.1% | __pw_methods_MOD_pw_scatter_p._omp_fn.24
||   1.1% |    500.4 |  27.6 |  5.4% | __fft_tools_MOD_yz_to_x._omp_fn.9
||   1.0% |    424.5 | 135.5 | 25.0% | __fast_MOD_zero_c3._omp_fn.0
||   1.0% |    424.0 |  16.0 |  3.7% | __qs_collocate_density_MOD_calculate_rho_elec._omp_fn.0
||============================================================================
|  30.1% | 13,298.4 |    -- |    -- | MPI
||----------------------------------------------------------------------------
||  11.4% |  5,022.7 | 186.3 |  3.7% | mpi_alltoallv
||  11.3% |  4,984.7 | 317.3 |  6.2% | MPI_ALLREDUCE
||   3.3% |  1,471.8 | 242.2 | 14.6% | MPI_WAITANY
||   1.4% |    638.8 |  86.2 | 12.3% | mpi_waitall
||   1.1% |    480.5 | 421.5 | 48.2% | mpi_bcast
||============================================================================
|  19.4% |  8,601.4 |    -- |    -- | ETC
||----------------------------------------------------------------------------
||   5.3% |  2,350.7 | 147.3 |  6.1% | apply
||   4.6% |  2,048.1 | 771.9 | 28.3% | __lll_timedwait_tid
||   2.3% |  1,023.1 | 388.9 | 28.4% | GOMP_parallel
||   2.3% |  1,006.1 | 182.9 | 15.9% | gomp_team_barrier_wait_end
||   1.3% |    566.9 |  24.1 |  4.2% | gotoblas_daxpy_k_sandybridge
||============================================================================
|   1.9% |    858.1 |    -- |    -- | RT
||----------------------------------------------------------------------------
||   1.9% |    847.2 | 111.8 | 12.0% | munmap
|=============================================================================

Notes for table 2:

  Table option:
    -O profile_max
  Options implied by table option:
    -d ti%@0.95,ti,imb_ti,imb_ti% -b fu,pe=[max,min],th=HIDE -s
    aggr_fu=max -s aggr_pe=max -s aggr_th=max -s total=HIDE

  Options for related tables:
    -O profile_pe_th           -O profile_pe.th       
    -O profile_th_pe           -O profile+src         
    -O load_balance            -O callers             
    -O callers+src             -O calltree            
    -O calltree+src        

  The Function value for Samp is the max of the PE values.
  The PE value for Samp is the max of the Thread values.
    (To specify different aggregations, see: pat_help report options s1)

  This table shows only lines with Samp% > 0.95.
    (To set thresholds to zero, specify:  -T)
  This table shows only the maximum, minimum PE entries, sorted by
    Samp.

  Imbalance percentages are relative to a set of threads or PEs.
  Other percentages at each level are of the Total for the program.
    (For percentages relative to next level up, specify:
      -s percent=r[elative])

  To make the profile easier to interpret, some samples are attributed
  to a caller that is either a user defined function, or a library
  function called directly by a user defined function.  To disable this
  adjustment, and show functions actually sampled, use the -P option.

Table 2:  Profile of maximum function times (limited entries shown)

  Samp% |     Samp |    Imb. |   Imb. | Function
        |          |    Samp |  Samp% |  PE=[max,min]
        |          |         |        |   Thread=HIDE
|-----------------------------------------------------------------------------
| 100.0% | 31,780.0 | 1,262.2 |   4.1% | gomp_barrier_wait_end
||----------------------------------------------------------------------------
|| 100.0% | 31,780.0 | 1,063.2 |   4.2% | pe.27
||  93.2% | 29,626.0 |   461.6 |   1.9% | pe.4
||============================================================================
|  16.7% |  5,302.0 |   317.3 |   6.2% | MPI_ALLREDUCE
||----------------------------------------------------------------------------
||  16.7% |  5,302.0 |      -- |     -- | pe.29
||  14.2% |  4,514.0 |      -- |     -- | pe.14
||============================================================================
|  16.4% |  5,209.0 |   186.3 |   3.7% | mpi_alltoallv
||----------------------------------------------------------------------------
||  16.4% |  5,209.0 |      -- |     -- | pe.31
||  14.9% |  4,742.0 |      -- |     -- | pe.14
||============================================================================
|  12.5% |  3,967.0 |   193.2 |   5.0% | __pw_methods_MOD_pw_integral_ab
||----------------------------------------------------------------------------
||  12.5% |  3,967.0 |      -- |     -- | pe.14
||  11.3% |  3,604.0 |      -- |     -- | pe.21
||============================================================================
|   8.9% |  2,820.0 |   771.9 |  28.3% | __lll_timedwait_tid
||----------------------------------------------------------------------------
||   8.9% |  2,820.0 |      -- |     -- | pe.27
||   4.2% |  1,324.0 |      -- |     -- | pe.17
||============================================================================
|   7.9% |  2,521.0 |   133.4 |   5.5% | apply
||----------------------------------------------------------------------------
||   7.9% |  2,521.0 |    26.5 |   1.3% | pe.19
||   7.1% |  2,244.0 |    42.2 |   2.3% | pe.28
||============================================================================
|   6.3% |  2,003.0 |    16.4 |   0.8% | __qmmm_gpw_energy_MOD_qmmm_elec_with_gaussian_lr
||----------------------------------------------------------------------------
||   6.3% |  2,003.0 |      -- |     -- | pe.6
||   6.2% |  1,974.0 |      -- |     -- | pe.28
||============================================================================
|   5.4% |  1,714.0 |   242.2 |  14.6% | MPI_WAITANY
||----------------------------------------------------------------------------
||   5.4% |  1,714.0 |      -- |     -- | pe.25
||   4.1% |  1,315.0 |      -- |     -- | pe.18
||============================================================================
|   5.3% |  1,676.0 |    10.3 |   0.6% | __qmmm_gpw_forces_MOD_qmmm_force_with_gaussian_low
||----------------------------------------------------------------------------
||   5.3% |  1,676.0 |      -- |     -- | pe.6
||   5.2% |  1,657.0 |      -- |     -- | pe.17
||============================================================================
|   4.6% |  1,470.0 |   179.4 |  12.6% | gomp_team_barrier_wait_end
||----------------------------------------------------------------------------
||   4.6% |  1,470.0 |   304.7 |  24.9% | pe.14
||   3.5% |  1,117.0 |    96.8 |  10.4% | pe.24
||============================================================================
|   4.4% |  1,412.0 |   388.9 |  28.4% | GOMP_parallel
||----------------------------------------------------------------------------
||   4.4% |  1,412.0 | 1,059.0 | 100.0% | pe.25
||   2.1% |    679.0 |   509.2 | 100.0% | pe.21
||============================================================================
|   4.1% |  1,307.0 |    35.7 |   2.8% | __pw_spline_utils_MOD_pw_nn_compose_r_no_pbc._omp_fn.0
||----------------------------------------------------------------------------
||   4.1% |  1,307.0 |   121.5 |  11.2% | pe.30
||   3.9% |  1,240.0 |    91.7 |   8.9% | pe.27
||============================================================================
|   3.7% |  1,161.0 |    92.6 |   8.2% | __kahan_sum_MOD_kahan_sum_d3
||----------------------------------------------------------------------------
||   3.7% |  1,161.0 |      -- |     -- | pe.14
||   3.1% |    986.0 |      -- |     -- | pe.31
||============================================================================
|   3.5% |  1,106.0 |    89.9 |   8.4% | __fast_MOD_zero_c2._omp_fn.1
||----------------------------------------------------------------------------
||   3.5% |  1,106.0 |    44.7 |   4.8% | pe.10
||   3.0% |    942.0 |    24.5 |   3.1% | pe.7
||============================================================================
|   3.4% |  1,081.0 |    47.7 |   4.6% | __fft_tools_MOD_x_to_yz._omp_fn.14
||----------------------------------------------------------------------------
||   3.4% |  1,081.0 |    56.7 |   6.3% | pe.26
||   3.1% |    987.0 |    19.3 |   2.4% | pe.19
||============================================================================
|   3.0% |    959.0 |   111.8 |  12.0% | munmap
||----------------------------------------------------------------------------
||   3.0% |    959.0 |   798.7 |  99.9% | pe.30
||   2.3% |    737.0 |   614.2 | 100.0% | pe.7
||============================================================================
|   2.8% |    902.0 |   421.5 |  48.2% | mpi_bcast
||----------------------------------------------------------------------------
||   2.8% |    902.0 |      -- |     -- | pe.5
||   0.4% |    133.0 |      -- |     -- | pe.0
||============================================================================
|   2.7% |    847.0 |    43.7 |   5.3% | __pw_methods_MOD_pw_zero
||----------------------------------------------------------------------------
||   2.7% |    847.0 |      -- |     -- | pe.27
||   2.4% |    761.0 |      -- |     -- | pe.11
||============================================================================
|   2.7% |    843.0 |     7.9 |   1.0% | __pw_spline_utils_MOD_add_fine2coarse
||----------------------------------------------------------------------------
||   2.7% |    843.0 |      -- |     -- | pe.13
||   2.6% |    826.0 |      -- |     -- | pe.8
||============================================================================
|   2.5% |    790.0 |     7.7 |   1.0% | __pw_spline_utils_MOD_add_coarse2fine
||----------------------------------------------------------------------------
||   2.5% |    790.0 |      -- |     -- | pe.12
||   2.4% |    776.0 |      -- |     -- | pe.4
||============================================================================
|   2.4% |    771.0 |    88.7 |  11.9% | __pw_methods_MOD_pw_axpy
||----------------------------------------------------------------------------
||   2.4% |    771.0 |      -- |     -- | pe.6
||   1.8% |    562.0 |      -- |     -- | pe.30
||============================================================================
|   2.3% |    725.0 |    86.2 |  12.3% | mpi_waitall
||----------------------------------------------------------------------------
||   2.3% |    725.0 |      -- |     -- | pe.16
||   1.7% |    527.0 |      -- |     -- | pe.11
||============================================================================
|   2.2% |    698.0 |    27.8 |   4.1% | __pw_methods_MOD_pw_scatter_p._omp_fn.24
||----------------------------------------------------------------------------
||   2.2% |    698.0 |    42.7 |   7.3% | pe.7
||   2.0% |    650.0 |    41.0 |   7.6% | pe.28
||============================================================================
|   2.1% |    682.0 |    19.5 |   3.0% | __pw_methods_MOD_pw_gather_p._omp_fn.22
||----------------------------------------------------------------------------
||   2.1% |    682.0 |    25.0 |   4.4% | pe.25
||   2.0% |    638.0 |     9.5 |   1.8% | pe.9
||============================================================================
|   1.9% |    599.0 |   188.5 |  32.5% | apply_extra_iter
||----------------------------------------------------------------------------
||   1.9% |    599.0 |    72.2 |  14.5% | pe.30
||   1.0% |    323.0 |    12.3 |   4.6% | pe.3
||============================================================================
|   1.9% |    591.0 |    17.3 |   3.0% | gotoblas_daxpy_k_sandybridge
||----------------------------------------------------------------------------
||   1.9% |    591.0 |    49.5 |  10.1% | pe.29
||   1.8% |    557.0 |    30.8 |   6.6% | pe.2
||============================================================================
|   1.8% |    560.0 |   103.6 |  19.1% | __fast_MOD_zero_c3._omp_fn.0
||----------------------------------------------------------------------------
||   1.8% |    560.0 |   165.2 |  35.4% | pe.9
||   1.1% |    358.0 |    56.0 |  18.8% | pe.30
||============================================================================
|   1.7% |    555.0 |    29.7 |   5.5% | __fft_tools_MOD_yz_to_x._omp_fn.9
||----------------------------------------------------------------------------
||   1.7% |    555.0 |    14.3 |   3.1% | pe.12
||   1.6% |    497.0 |    13.0 |   3.1% | pe.30
||============================================================================
|   1.4% |    443.0 |    10.5 |   2.4% | __qs_collocate_density_MOD_calculate_rho_elec._omp_fn.0
||----------------------------------------------------------------------------
||   1.4% |    443.0 |     6.5 |   1.8% | pe.28
||   1.3% |    422.0 |     3.7 |   1.0% | pe.23
||============================================================================
|   1.1% |    360.0 |    52.8 |  15.1% | __pw_methods_MOD_pw_copy._omp_fn.46
||----------------------------------------------------------------------------
||   1.1% |    360.0 |    60.5 |  20.2% | pe.12
||   0.8% |    255.0 |    22.2 |  10.4% | pe.15
||============================================================================
|   1.0% |    324.0 |     7.1 |   2.3% | __pw_poisson_methods_MOD_pw_poisson_solve
||----------------------------------------------------------------------------
||   1.0% |    324.0 |      -- |     -- | pe.7
||   0.9% |    297.0 |      -- |     -- | pe.31
||============================================================================
|   1.0% |    321.0 |    67.3 |  21.6% | mpi_isend
||----------------------------------------------------------------------------
||   1.0% |    321.0 |      -- |     -- | pe.11
||   0.5% |    156.0 |      -- |     -- | pe.28
||============================================================================
|   1.0% |    317.0 |    34.0 |  11.1% | __pw_spline_utils_MOD_find_coeffs
||----------------------------------------------------------------------------
||   1.0% |    317.0 |      -- |     -- | pe.10
||   0.8% |    256.0 |      -- |     -- | pe.31
|=============================================================================

===================  Observations and suggestions  ===================

MPI Grid Detection:

    There appears to be point-to-point MPI communication in a 8 X 2 X 2
    grid pattern. The 30.1% of the total execution time spent in MPI
    functions might be reduced with a rank order that maximizes
    communication between ranks on the same node. The effect of several
    rank orders is estimated below.

    No custom rank order was found that is better than the SMP order.

    Rank Order    On-Node    On-Node  MPICH_RANK_REORDER_METHOD
                 Bytes/PE  Bytes/PE%  
                            of Total  
                            Bytes/PE  

           SMP  1.881e+11     34.66%  1
          Fold  7.171e+10     13.21%  2
    RoundRobin  4.957e+10      9.13%  0


Metric-Based Rank Order:

    No rank order was suggested based on the USER Samp metric because
    that metric was already well balanced across the nodes.


MPI utilization:

    The time spent processing MPI communications is relatively high. 
    Functions and callsites responsible for consuming the most time can
    be found in the table generated by pat_report -O callers+src (within
    the MPI group).

=========================  End Observations  =========================

Notes for table 3:

  Table option:
    -O samp_profile+src
  Options implied by table option:
    -d sa%@0.95,sa,imb_sa,imb_sa% -b gr,fu,so,li,pe=HIDE,th=HIDE

  Options for related tables:
    -O samp_profile            -O profile_max         

  The Total value for Samp is the sum of the Group values.
  The Group value for Samp is the sum of the Function values.
  The Function value for Samp is the sum of the Source values.
  The Source value for Samp is the sum of the Line values.
  The Line value for Samp is the avg of the PE values.
  The PE value for Samp is the value for the main thread only.
    (If the main thread is atypical, try the option -s aggr_th=sum.)
    (To specify different aggregations, see: pat_help report options s1)

  This table shows only lines with Samp% > 0.95.
    (To set thresholds to zero, specify:  -T)

  Imbalance percentages are relative to a set of threads or PEs.
  Other percentages at each level are of the Total for the program.
    (For percentages relative to next level up, specify:
      -s percent=r[elative])

  To make the profile easier to interpret, some samples are attributed
  to a caller that is either a user defined function, or a library
  function called directly by a user defined function.  To disable this
  adjustment, and show functions actually sampled, use the -P option.
  
  The following groups were pruned due to thresholding:
    IO, HEAP, PTHREAD, OMP, MATH, STDIO, BLAS, LAPACK, BLACS, SCALAPACK,
    FFTW, PBLAS, STRING, FS

Table 3:  Profile by Group, Function, and Line

  Samp% |     Samp |  Imb. |  Imb. | Group
        |          |  Samp | Samp% |  Function
        |          |       |       |   Source
        |          |       |       |    Line
        |          |       |       |     PE=HIDE
        |          |       |       |      Thread=HIDE
       
 100.0% | 44,246.0 |    -- |    -- | Total
|-----------------------------------------------------------------------------
|  48.1% | 21,266.5 |    -- |    -- | USER
||----------------------------------------------------------------------------
||   8.5% |  3,773.8 |    -- |    -- | __pw_methods_MOD_pw_integral_ab
3|        |          |       |       |  cp2k-6.1.0/src/pw/pw_methods.F
4|   8.0% |  3,551.4 | 190.6 |  5.3% |   line.1742
||   4.5% |  1,986.6 |    -- |    -- | __qmmm_gpw_energy_MOD_qmmm_elec_with_gaussian_lr
3|        |          |       |       |  BioExcel2/cp2k-6.1.0/src/qmmm_gpw_energy.F
4|   2.1% |    926.5 |  37.5 |  4.0% |   line.833
||   3.8% |  1,665.7 |    -- |    -- | __qmmm_gpw_forces_MOD_qmmm_force_with_gaussian_low
3|        |          |       |       |  BioExcel2/cp2k-6.1.0/src/qmmm_gpw_forces.F
4|   2.2% |    960.5 |  45.5 |  4.7% |   line.1296
||   2.9% |  1,271.3 |    -- |    -- | __pw_spline_utils_MOD_pw_nn_compose_r_no_pbc._omp_fn.0
3|        |          |       |       |  cp2k-6.1.0/src/pw/pw_spline_utils.F
||   2.4% |  1,068.4 |    -- |    -- | __kahan_sum_MOD_kahan_sum_d3
3|        |          |       |       |  obj/ARCHER_craypat/psmp/kahan_sum.F90
4|   2.2% |    963.5 |  99.5 |  9.7% |   line.534
||   2.3% |  1,031.0 |    -- |    -- | __fft_tools_MOD_x_to_yz._omp_fn.14
3|        |          |       |       |  cp2k-6.1.0/src/pw/fft_tools.F
4|   2.2% |    967.8 |  85.2 |  8.3% |   line.1471
||   2.2% |    990.0 | 116.0 | 10.8% | __fast_MOD_zero_c2._omp_fn.1
3|        |          |       |       |  obj/ARCHER_craypat/psmp/fast.F90
4|        |          |       |       |   line.168
||   1.9% |    835.1 |    -- |    -- | __pw_spline_utils_MOD_add_fine2coarse
3|        |          |       |       |  cp2k-6.1.0/src/pw/pw_spline_utils.F
||   1.8% |    803.3 |    -- |    -- | __pw_methods_MOD_pw_zero
3|        |          |       |       |  cp2k-6.1.0/src/pw/pw_methods.F
4|   1.7% |    749.2 |  38.8 |  5.1% |   line.103
||   1.8% |    782.3 |    -- |    -- | __pw_spline_utils_MOD_add_coarse2fine
3|        |          |       |       |  cp2k-6.1.0/src/pw/pw_spline_utils.F
||   1.5% |    682.3 |    -- |    -- | __pw_methods_MOD_pw_axpy
3|        |          |       |       |  cp2k-6.1.0/src/pw/pw_methods.F
4|   1.3% |    584.6 |  87.4 | 13.4% |   line.741
||   1.4% |    609.1 |    -- |    -- | __pw_methods_MOD_pw_gather_p._omp_fn.22
3|        |          |       |       |  cp2k-6.1.0/src/pw/pw_methods.F
4|   1.3% |    559.2 |  15.8 |  2.8% |   line.1021
||   1.2% |    550.1 |    -- |    -- | __pw_methods_MOD_pw_scatter_p._omp_fn.24
3|        |          |       |       |  cp2k-6.1.0/src/pw/pw_methods.F
4|   1.0% |    450.0 |  39.0 |  8.2% |   line.1211
||   1.1% |    500.4 |    -- |    -- | __fft_tools_MOD_yz_to_x._omp_fn.9
3|        |          |       |       |  cp2k-6.1.0/src/pw/fft_tools.F
4|   1.1% |    499.8 |  27.2 |  5.3% |   line.1551
||   1.0% |    424.5 | 135.5 | 25.0% | __fast_MOD_zero_c3._omp_fn.0
3|        |          |       |       |  obj/ARCHER_craypat/psmp/fast.F90
4|        |          |       |       |   line.184
||   1.0% |    424.0 |    -- |    -- | __qs_collocate_density_MOD_calculate_rho_elec._omp_fn.0
3|        |          |       |       |  BioExcel2/cp2k-6.1.0/src/qs_collocate_density.F
4|   1.0% |    423.8 |  15.2 |  3.6% |   line.1712
||============================================================================
|  30.1% | 13,298.4 |    -- |    -- | MPI
||----------------------------------------------------------------------------
||  11.4% |  5,022.7 | 186.3 |  3.7% | mpi_alltoallv
||  11.3% |  4,984.7 | 317.3 |  6.2% | MPI_ALLREDUCE
||   3.3% |  1,471.8 | 242.2 | 14.6% | MPI_WAITANY
||   1.4% |    638.8 |  86.2 | 12.3% | mpi_waitall
||   1.1% |    480.5 | 421.5 | 48.2% | mpi_bcast
||============================================================================
|  19.4% |  8,601.4 |    -- |    -- | ETC
||----------------------------------------------------------------------------
||   5.3% |  2,350.7 | 147.3 |  6.1% | apply
||   4.6% |  2,048.1 | 771.9 | 28.3% | __lll_timedwait_tid
3|        |          |       |       |  sysv/linux/x86_64/lowlevellock.S
4|        |          |       |       |   line.434
||   2.3% |  1,023.1 |    -- |    -- | GOMP_parallel
3|        |          |       |       |  ../cray-gcc-6.3.0-201701050407.93fe37becc347/libgomp/parallel.c
4|   2.0% |    885.2 | 382.8 | 31.2% |   line.168
||   2.3% |  1,006.1 |    -- |    -- | gomp_team_barrier_wait_end
3|   2.2% |    978.6 |    -- |    -- |  config/linux/x86/futex.h
4|   1.4% |    608.6 | 173.4 | 22.9% |   line.39
||   1.3% |    566.9 |  24.1 |  4.2% | gotoblas_daxpy_k_sandybridge
||============================================================================
|   1.9% |    858.1 |    -- |    -- | RT
||----------------------------------------------------------------------------
||   1.9% |    847.2 | 111.8 | 12.0% | munmap
3|        |          |       |       |  ../sysdeps/unix/syscall-template.S
4|        |          |       |       |   line.82
|=============================================================================

Notes for table 4:

  Table option:
    -O hwpc
  Options implied by table option:
    -d P -b pe=HIDE,th=HIDE -s show_data=rows

  The Total value for each data item is the avg of the PE values.
  The PE value for each data item is the value for the main thread only.
    (If the main thread is atypical, try the option -s aggr_th=sum.)
    (To specify different aggregations, see: pat_help report options s1)

  
  Percent of peak FP Ops based on a compute unit.
  
  Collection of each of the following performance counters was
  attempted, but all of the values recorded in the data file were zero,
  so the counter and all derived counters that depend on it were filtered
  from this table:
    FP_COMP_OPS_EXE:SSE_PACKED_SINGLE
    SIMD_FP_256:PACKED_SINGLE

Table 4:  Program HW Performance Counter Data

PE=HIDE / Thread=HIDE

  
==============================================================================
  Avg of PE values
------------------------------------------------------------------------------
  CPU_CLK_UNHALTED:THREAD_P                     1,091,983,822,724 
  CPU_CLK_UNHALTED:REF_P                           33,982,491,599 
  DTLB_LOAD_MISSES:MISS_CAUSES_A_WALK                 852,550,080 
  DTLB_STORE_MISSES:MISS_CAUSES_A_WALK                706,252,232 
  L1D:REPLACEMENT                                  18,482,387,737 
  L2_RQSTS:ALL_DEMAND_DATA_RD                      12,289,923,212 
  L2_RQSTS:DEMAND_DATA_RD_HIT                       7,353,552,707 
  FP_COMP_OPS_EXE:SSE_SCALAR_DOUBLE               254,090,619,850 
  FP_COMP_OPS_EXE:SSE_FP_SCALAR_SINGLE                 36,013,491 
  FP_COMP_OPS_EXE:X87                              22,780,380,130 
  FP_COMP_OPS_EXE:SSE_FP_PACKED_DOUBLE              2,140,504,488 
  SIMD_FP_256:PACKED_DOUBLE                        16,876,613,544 
  PM_ENERGY:NODE                   51.197 /sec             22,792 J
  CPU_CLK                            3.21GHz                      
  HW FP Ops / User time           783.262M/sec    348,694,476,624 ops  4.0%peak(DP)
  Total SP ops                      0.081M/sec         36,013,491 ops
  Total DP ops                    783.181M/sec    348,658,463,133 ops
  MFLOPS (aggregate)            25,064.39M/sec                    
  D2 cache hit,miss ratio           73.3% hits              26.7% misses
  D2 to D1 bandwidth            1,684.967MiB/sec  786,555,085,580 bytes
==============================================================================

Notes for table 5:

  Table option:
    -O program_energy
  Options implied by table option:
    -d PM_ENERGY:NODE,Pn,PM_ENERGY:ACC,Pa -b ni=[mmm],pe.th=HIDE -s
    aggr_ni_PM_ENERGY:ACC=sum -s aggr_ni_PM_ENERGY:NODE=sum -s
    aggr_pe.th_PM_ENERGY:ACC=max -s aggr_pe.th_PM_ENERGY:NODE=max

  The Total value for PM_ENERGY:NODE is the sum of the Node Id values.
  The Node Id value for PM_ENERGY:NODE is the max of the PE.Thread values.
    (To specify different aggregations, see: pat_help report options s1)

  This table shows only the maximum, median, minimum Node Id entries,
    sorted by PM_ENERGY:NODE.

Table 5:
  Program energy and power usage (from Cray PM) (limited entries shown)

 PM_ENERGY |     Power | Node Id=[mmm]
 :NODE (J) | :node (W) |  PE.Thread=HIDE
          
   729,341 | 1,617.180 | Total
|---------------------------------------
|    95,964 |   211.241 | nid.4599
|    90,394 |   200.182 | nid.4611
|    88,062 |   197.216 | nid.4454
|=======================================

Notes for table 6:

  Table option:
    -O himem
  Options implied by table option:
    -d hm,Hm -b nn,pe=HIDE -s sort_by_nn=yes

  The Total value for each data item is the sum of the Numanode values.
  The Numanode value for each data item is the avg of the PE values.
    (To specify different aggregations, see: pat_help report options s1)

  The value shown for Process HiMem is calculated from information in
  the /proc/self/numa_maps files captured near the end of the program. 
  It is the total size of all pages, including huge pages, that were
  actually mapped into physical memory from both private and shared
  memory segments.


Table 6:  Memory High Water Mark by Numa Node

  Process |    HiMem |    HiMem | Numanode
    HiMem |     Numa |     Numa |  PE=HIDE
 (MBytes) |   Node 0 |   Node 1 | 
          | (MBytes) | (MBytes) | 
         
    988.6 |    539.1 |    449.4 | Total
|-------------------------------------------
|    520.0 |    519.3 |      0.7 | numanode.0
|    468.6 |     19.8 |    448.8 | numanode.1
|===========================================

Notes for table 7:

  Table option:
    -O program_time
  Options implied by table option:
    -d pt,hm -b pe=[mmm],th

  The Total value for Process HiMem (MBytes), Process Time is the avg of the PE values.
  The PE value for Process HiMem (MBytes), Process Time is the value for the main thread only.
    (If the main thread is atypical, try the option -s aggr_th=sum.)
    (To specify different aggregations, see: pat_help report options s1)

  The value shown for Process HiMem is calculated from information in
  the /proc/self/numa_maps files captured near the end of the program. 
  It is the total size of all pages, including huge pages, that were
  actually mapped into physical memory from both private and shared
  memory segments.

  This table shows only the maximum, median, minimum PE entries,
    sorted by Process Time.

Table 7:  Wall Clock Time, Memory High Water Mark (limited entries shown)

    Process |  Process | PE=[mmm]
       Time |    HiMem |  Thread
            | (MBytes) | 
           
 450.995434 |    494.3 | Avg of PE values
|----------------------------------------
| 456.320038 |    331.3 | pe.27
|            |          |  thread.0
| 451.654046 |    435.1 | pe.1
|            |          |  thread.0
| 441.116612 |    684.6 | pe.17
|            |          |  thread.0
|========================================

========================  Additional details  ========================

Experiment:  samp_cs_time

Sampling interval:  10000 microsecs

Original path to data file:
  /fs3/d118/d118/fionabio/BioExcel2/cp2k_benchmarks/QMMM/QM/8_nodes/test-1/cp2k.psmp+samp+38276-4453s.xf  (RTS)

Original program:
  /home3/d118/d118/fionabio/BioExcel2/cp2k-6.1.0/exe/ARCHER_craypat/cp2k.psmp

Instrumented with:  pat_build -o cp2k.psmp+samp cp2k.psmp

  Option file "apa" contained:
    -Drtenv=PAT_RT_PERFCTR=default_samp
    -Drtenv=PAT_RT_EXPERIMENT=samp_cs_time
    -Drtenv=PAT_RT_SAMPLING_MODE=3
    -g upc
    -g caf
    -g mpi
    -g shmem
    -g syscall

Instrumented program:
  /home/d118/d118/fionabio/BioExcel2/cp2k-6.1.0/exe/ARCHER_craypat/cp2k.psmp+samp

Program invocation:
  /home/d118/d118/fionabio/BioExcel2/cp2k-6.1.0/exe/ARCHER_craypat/cp2k.psmp+samp qmmm-1.inp

Exit Status:  0 for 32 PEs

Thread start functions and creator functions:
    32 threads:  main
   160 threads:  gomp_thread_start <- gomp_team_start

Memory pagesize:  4 KiB

Memory hugepagesize:  Not Available

Programming environment:  GNU

Runtime environment variables:
  ATP_HOME=/opt/cray/atp/2.1.0
  ATP_IGNORE_SIGTERM=1
  ATP_MRNET_COMM_PATH=/opt/cray/atp/2.1.0/libexec/atp_mrnet_commnode_wrapper
  ATP_POST_LINK_OPTS=-Wl,-L/opt/cray/atp/2.1.0/libApp/
  CRAYOS_VERSION=5.2.82
  CRAYPE_VERSION=2.5.10
  CRAY_FFTW_VERSION=3.3.6.3
  CRAY_LIBSCI_VERSION=16.11.1
  DVS_VERSION=0.9.0
  FFTW_VERSION=3.3.6.3
  GCC_VERSION=6.3.0
  GNU_VERSION=6.3.0
  LIBSCI_VERSION=16.11.1
  MODULE_VERSION=3.2.10.6
  MODULE_VERSION_STACK=3.2.10.6
  MPICH_ABORT_ON_ERROR=1
  MPICH_DIR=/opt/cray/mpt/7.5.5/gni/mpich-gnu/5.1
  OMP_NUM_THREADS=6
  OMP_STACKSIZE=50M
  PATH=/opt/cray/perftools/6.4.6/bin:/opt/cray/papi/5.5.1.1/bin:/opt/cray/llm/default/bin:/opt/cray/llm/default/etc:/opt/cray/xpmem/0.1-2.0502.64982.7.26.ari/bin:/opt/cray/lustre-cray_ari_s/2.5_3.0.101_0.46.1_1.0502.8871.43.1-1.0502.21728.74.6/sbin:/opt/cray/lustre-cray_ari_s/2.5_3.0.101_0.46.1_1.0502.8871.43.1-1.0502.21728.74.6/bin:/opt/cray/alps/5.2.5-2.0502.9955.44.1.ari/sbin:/opt/cray/alps/5.2.5-2.0502.9955.44.1.ari/bin:/opt/cray/sdb/1.1-1.0502.63652.4.25.ari/bin:/opt/cray/nodestat/2.2-1.0502.60539.1.31.ari/bin:/opt/cray/fftw/3.3.6.3/ivybridge/bin:/opt/cray/rca/1.0.0-2.0502.60530.1.62.ari/bin:/opt/cray/alps/5.2.4-2.0502.9774.31.11.ari/sbin:/opt/cray/dvs/2.5_0.9.0-1.0502.2188.1.116.ari/bin:/opt/cray/xpmem/0.1-2.0502.64982.5.3.ari/bin:/opt/cray/ugni/6.0-1.0502.10863.8.29.ari/bin:/opt/cray/udreg/2.3.2-1.0502.10518.2.17.ari/bin:/opt/gcc/6.3.0/bin:/home/y07/y07/cse/epcc-archer-tools/bin:/home/y07/y07/cse/git/git-2.16.2_build1/install/bin:/home/y07/y07/cse/curl/curl-7.58.0_build1/install/bin:/home/y07/y07/cse/openssl/openssl-1.1.0g_build1/install/bin:/home/y07/y07/cse/xalt/0.6.0/libexec:/home/y07/y07/cse/xalt/0.6.0/bin:/usr/local/packages/cse/ack:/usr/local/packages/cse/quickstart/1.0:/home/y07/y07/cse/leave_time/1.3.0:/home/y07/y07/cse/nano/2.2.6/bin:/usr/local/packages/cse/bolt/0.6/bin:/opt/cray/mpt/7.5.5/gni/bin:/opt/pbs/12.2.401.141761/bin:/opt/cray/craype/2.5.10/bin:/opt/cray/switch/1.0-1.0502.60522.1.61.ari/bin:/opt/cray/eslogin/eswrap/1.3.3-1.020200.1280.0/bin:/opt/modules/3.2.10.6/bin:/usr/local/bin:/usr/bin:/bin:/usr/bin/X11:/usr/X11R6/bin:/usr/games:/usr/lib/mit/bin:/usr/lib/mit/sbin:/sbin:/usr/sbin:.:/usr/lib/qt3/bin:/opt/cray/bin
  PAT_BUILD_PAPI_BASEDIR=/opt/cray/papi/5.5.1.1
  PAT_REPORT_PRUNE_NAME=_cray$mt_start_,__cray_hwpc_,f_cray_hwpc_,cstart,__pat_,pat_region_,PAT_,OMP.slave_loop,slave_entry,_new_slave_entry,THREAD_POOL_join,__libc_start_main,_start,__start,start_thread,__wrap_,UPC_ADIO_,_upc_,upc_,__caf_,__pgas_,syscall
  PAT_RT_EXPERIMENT=samp_cs_time
  PAT_RT_PERFCTR=default_samp
  PAT_RT_SAMPLING_MODE=3
  PERFTOOLS_VERSION=6.4.6
  PMI_FORK_RANK=0
  PMI_GNI_COOKIE=2015232000:2015297536
  PMI_GNI_DEV_ID=0
  PMI_GNI_LOC_ADDR=5861:5861
  PMI_GNI_PTAG=156:157
  XTOS_VERSION=5.2.82

Report time environment variables:
    CRAYPAT_ROOT=/opt/cray/perftools/6.4.6
    PAT_REPORT_PRUNE_NAME=_cray$mt_start_,__cray_hwpc_,f_cray_hwpc_,cstart,__pat_,pat_region_,PAT_,OMP.slave_loop,slave_entry,_new_slave_entry,THREAD_POOL_join,__libc_start_main,_start,__start,start_thread,__wrap_,UPC_ADIO_,_upc_,upc_,__caf_,__pgas_,syscall

Number of MPI control variables collected:  104

  (To see the list, specify: -s mpi_cvar=show)

Report command line options:
  -o craypat_qm_8_nodes_32_mpiprocs_6_threads_5_steps.txt

Operating system:
  Linux 3.0.101-0.46.1_1.0502.8871-cray_ari_c #1 SMP Mon Oct 8 17:27:42 UTC 2018

Hardware performance counter events:
   CPU_CLK_UNHALTED:THREAD_P             Cycles when processor is not in halted state:Cycles when thread is not halted
   CPU_CLK_UNHALTED:REF_P                Cycles when processor is not in halted state:Cycles when the core is unhalted (count at 100 Mhz)
   DTLB_LOAD_MISSES:MISS_CAUSES_A_WALK   Data TLB load misses:Demand load miss in all TLB levels which causes a page walk of anypage size
   DTLB_STORE_MISSES:MISS_CAUSES_A_WALK  Data TLB store misses:Miss in all TLB levels that causes a page walk of any page size (4K/2M/4M/1G)
   L1D:REPLACEMENT                       L1D cache:Number of cache lines brought into the L1D cache
   L2_RQSTS:ALL_DEMAND_DATA_RD           L2 requests:Demand  data read requests to L2 cache
   L2_RQSTS:DEMAND_DATA_RD_HIT           L2 requests:Demand data read requests that hit L2
   FP_COMP_OPS_EXE:SSE_SCALAR_DOUBLE     Counts number of floating point events:Number of SSE or AVX-128 double precision FP scalar uops executed
   FP_COMP_OPS_EXE:SSE_FP_SCALAR_SINGLE  Counts number of floating point events:Number of SSE or AVX-128 single precision FP scalar uops executed
   FP_COMP_OPS_EXE:X87                   Counts number of floating point events:Number of X87 uops executed
   FP_COMP_OPS_EXE:SSE_PACKED_SINGLE     Counts number of floating point events:Number of SSE or AVX-128 single precision FP packed uops executed
   FP_COMP_OPS_EXE:SSE_FP_PACKED_DOUBLE  Counts number of floating point events:Number of SSE or AVX-128 double precision FP packed uops executed
   SIMD_FP_256:PACKED_SINGLE             Counts 256-bit packed floating point instructions:Counts 256-bit packed single-precision
   SIMD_FP_256:PACKED_DOUBLE             Counts 256-bit packed floating point instructions:Counts 256-bit packed double-precision
   PM_ENERGY:NODE                        Compute node accumulated energy

  This set of HWPC events requires multiplexing, which reduces
  the accuracy of the data collected. If the best possible
  accuracy is needed, you should rerun to collect data for
  smaller sets of events, that do not require multiplexing.

Number of traced functions:  355

  (To see the list, specify:  -s traced_functions=show)

